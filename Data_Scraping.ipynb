{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "528a87d6-0f7f-40dd-8835-8c373b2f03a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped page 1\n",
      "craped total 0 products across 1 pages\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Setup Chrome\n",
    "opti2ons = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Flipkart URL\n",
    "url = \"https://www.flipkart.com/search?q=WATCHES&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off\"\n",
    "driver.get(url)\n",
    "\n",
    "watches = []\n",
    "num_pages = 1  # adjust pages as needed\n",
    "\n",
    "for page in range(num_pages):\n",
    "    time.sleep(5)  # wait for page load\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    for watch in soup.select(\"div.DOjaWF gdgoEp\"):\n",
    "        name = watch.select_one(\"a.WKTcLC\")\n",
    "        price = watch.select_one(\"div.Nx9bqj\")\n",
    "        brand = watch.select_one(\"div.syl9yP\")\n",
    "\n",
    "        watches.append({\n",
    "            \"Name\": name.get_text(strip=True) if name else \"n/a\",\n",
    "            \"Price\": price.get_text(strip=True) if price else \"n/a\",\n",
    "            \"Brand\": brand.get_text(strip=True) if price else \"n/a\",\n",
    "        })\n",
    "\n",
    "    print(f\"Scraped page {page+1}\")\n",
    "\n",
    "    try:\n",
    "        next_btn = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//a[@class='_9QVEpD']/span[text()='Next']\"))\n",
    "        )\n",
    "        driver.execute_script(\"arguments[0].click();\", next_btn)\n",
    "    except:\n",
    "        print(\"No more pages available.\")\n",
    "        break\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(watches)\n",
    "df.to_csv(\"flipkart_watchs.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"Scraped total {len(df)} watches across {page+1} pages\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b88fbd-8775-4099-b643-7f37f276c8a5",
   "metadata": {},
   "source": [
    "## DATA SCRAPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84c10179-c226-42f2-8a20-c3b89b6e4394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped page 1\n",
      "Scraped page 2\n",
      "Scraped page 3\n",
      "Scraped page 4\n",
      "Scraped page 5\n",
      "Scraped page 6\n",
      "Scraped page 7\n",
      "Scraped page 8\n",
      "Scraped page 9\n",
      "Scraped page 10\n",
      "Scraped page 11\n",
      "Scraped page 12\n",
      "Scraped page 13\n",
      "Scraped page 14\n",
      "Scraped page 15\n",
      "Scraped page 16\n",
      "Scraped page 17\n",
      "Scraped page 18\n",
      "Scraped page 19\n",
      "Scraped page 20\n",
      "Scraped page 21\n",
      "Scraped page 22\n",
      "Scraped page 23\n",
      "Scraped page 24\n",
      "Scraped page 25\n",
      "Scraped page 26\n",
      "Scraped page 27\n",
      "Scraped page 28\n",
      "Scraped page 29\n",
      "Scraped page 30\n",
      "Scraped page 31\n",
      "Scraped page 32\n",
      "Scraped page 33\n",
      "Scraped page 34\n",
      "Scraped page 35\n",
      "Scraped page 36\n",
      "Scraped page 37\n",
      "Scraped page 38\n",
      "Scraped page 39\n",
      "Scraped page 40\n",
      "Scraped page 41\n",
      "Scraped page 42\n",
      "Scraped page 43\n",
      "Scraped page 44\n",
      "Scraped page 45\n",
      "Scraped page 46\n",
      "Scraped page 47\n",
      "Scraped page 48\n",
      "Scraped page 49\n",
      "Scraped page 50\n",
      "Scraped page 51\n",
      "Scraped page 52\n",
      "Scraped page 53\n",
      "Scraped page 54\n",
      "Scraped page 55\n",
      "Scraped page 56\n",
      "Scraped page 57\n",
      "Scraped page 58\n",
      "Scraped page 59\n",
      "Scraped page 60\n",
      "Scraped page 61\n",
      "Scraped page 62\n",
      "Scraped page 63\n",
      "Scraped page 64\n",
      "Scraped page 65\n",
      "Scraped page 66\n",
      "Scraped page 67\n",
      "Scraped page 68\n",
      "Scraped page 69\n",
      "Scraped page 70\n",
      "Scraped page 71\n",
      "Scraped total 2840 watches across 71 pages\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Setup Chrome\n",
    "options = Options()\n",
    "# options.add_argument(\"--headless\")   # disable headless for testing\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Flipkart URL\n",
    "url = \"https://www.flipkart.com/search?q=watches\"\n",
    "driver.get(url)\n",
    "\n",
    "watches = []\n",
    "num_pages = 71   \n",
    "\n",
    "for page in range(num_pages):\n",
    "    time.sleep(5)  # wait for page load\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    for watch in soup.select(\"div[data-id]\"):\n",
    "        name = watch.select_one(\"a.IRpwTa\") or watch.select_one(\"a.WKTcLC\")\n",
    "        price = watch.select_one(\"div.Nx9bqj\")\n",
    "        brand = watch.select_one(\"div.syl9yP\")\n",
    "\n",
    "        watches.append({\n",
    "            \"Name\": name.get_text(strip=True) if name else \"n/a\",\n",
    "            \"Price\": price.get_text(strip=True) if price else \"n/a\",\n",
    "            \"Brand\": brand.get_text(strip=True) if brand else \"n/a\",\n",
    "        })\n",
    "\n",
    "    print(f\"Scraped page {page+1}\")\n",
    "\n",
    "    try:\n",
    "        next_btn = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//a/span[text()='Next']\"))\n",
    "        )\n",
    "        driver.execute_script(\"arguments[0].click();\", next_btn)\n",
    "    except:\n",
    "        print(\"No more pages available.\")\n",
    "        break\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(watches)\n",
    "df.to_csv(\"flipkart_watches_data.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"Scraped total {len(df)} watches across {page+1} pages\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fc42fef-ead5-4622-8b5b-39f512e69e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df[\"Ratings\"] = np.round(np.random.uniform(1, 5, size=len(df)),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d246e2a3-7c30-41a5-ac98-5dda9cbe29e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Reviews\"] = np.random.randint(0, 401, size=len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70f2378e-c66e-41fd-b1f4-c909e6754b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"flipkart_watches_data2k.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35689c1c-78cd-4733-a68a-3fbde8e16450",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
